{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage of tu y usted, vosotros y ustedes in American and Peninsular Spanish"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: Nicole Dodd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, re\n",
    "from nltk.corpus import SpanishReader\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overall analysis of American vs. Peninsular usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load corpora files\n",
    "\n",
    "american_corpus = []\n",
    "peninsular_corpus = []\n",
    "\n",
    "corpora_dir = 'C:/Users/nicol/OneDrive/Documents/Education/Graduate - UCD/2020 Summer/GSR Spanish Corpus/corpora-files/'\n",
    "corpora_files = os.listdir(corpora_dir)\n",
    "\n",
    "for file in corpora_files:\n",
    "    myspreader = SpanishReader.SpanishPlaintextCorpusReader(corpora_dir, file)\n",
    "    text = myspreader.words()\n",
    "    if re.search('a-xix-', file) or re.search('a-xx-', file):\n",
    "        for t in text:\n",
    "            american_corpus.append(t)\n",
    "    else: # if peninsular\n",
    "        for t in text:\n",
    "            peninsular_corpus.append(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1922409\n",
      "1686552\n"
     ]
    }
   ],
   "source": [
    "## Get size of corpora\n",
    "\n",
    "a_corp_n = len(american_corpus)\n",
    "print(a_corp_n)\n",
    "p_corp_n = len(peninsular_corpus)\n",
    "print(p_corp_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Search for ust/vos in corpora\n",
    "\n",
    "def count_word(word, corpus):\n",
    "    count = 0\n",
    "    for c in corpus:\n",
    "        if c.lower() == word:\n",
    "            count += 1\n",
    "    \n",
    "    return count\n",
    "\n",
    "tu = ['tu']\n",
    "usted = ['usted', 'vd.', 'ud.']\n",
    "vosotros = ['vosotros', 'vosotras']\n",
    "ustedes = ['ustedes', 'vds.']\n",
    "yo = ['yo']\n",
    "nosotros = ['nosotros', 'nosotras', 'nos.']\n",
    "\n",
    "a_tu = 0\n",
    "p_tu = 0\n",
    "a_usted = 0\n",
    "p_usted = 0\n",
    "a_vosotros = 0\n",
    "p_vosotros = 0\n",
    "a_ustedes = 0\n",
    "p_ustedes = 0\n",
    "a_yo = 0\n",
    "p_yo = 0\n",
    "a_nosotros = 0\n",
    "p_nosotros = 0\n",
    "\n",
    "for word in tu:\n",
    "    a_tu += count_word(word, american_corpus)\n",
    "    p_tu += count_word(word, peninsular_corpus)\n",
    "\n",
    "for word in usted:\n",
    "    a_usted += count_word(word, american_corpus)\n",
    "    p_usted += count_word(word, peninsular_corpus)\n",
    "\n",
    "for word in vosotros:\n",
    "    a_vosotros += count_word(word, american_corpus)\n",
    "    p_vosotros += count_word(word, peninsular_corpus)\n",
    "\n",
    "for word in ustedes:\n",
    "    a_ustedes += count_word(word, american_corpus)\n",
    "    p_ustedes += count_word(word, peninsular_corpus)\n",
    "    \n",
    "for word in yo:\n",
    "    a_yo += count_word(word, american_corpus)\n",
    "    p_yo += count_word(word, peninsular_corpus)\n",
    "    \n",
    "for word in nosotros:\n",
    "    a_nosotros += count_word(word, american_corpus)\n",
    "    p_nosotros += count_word(word, peninsular_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          American  Peninsular\n",
      "tu            1176         782\n",
      "usted         2250        3117\n",
      "vosotros        97          90\n",
      "ustedes        377         329\n",
      "yo            4442        4016\n",
      "nosotros       822         636\n",
      "\n",
      "Size of American corpus: 1922409\n",
      "\n",
      "Size of Peninsular corpus: 1686552\n",
      "\n",
      "American      9164\n",
      "Peninsular    8970\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "## Get total counts\n",
    "\n",
    "counts_dict = {'American': [a_tu, a_usted, a_vosotros, a_ustedes, a_yo, a_nosotros], \n",
    "               'Peninsular': [p_tu, p_usted, p_vosotros, p_ustedes, p_yo, p_nosotros]}\n",
    "\n",
    "total_counts = pd.DataFrame(counts_dict, index = ['tu', 'usted', 'vosotros', 'ustedes', 'yo', 'nosotros'] )\n",
    "\n",
    "print(total_counts)\n",
    "print('\\nSize of American corpus: ' + str(a_corp_n))\n",
    "print('\\nSize of Peninsular corpus: ' + str(p_corp_n) +'\\n')\n",
    "print(total_counts.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               American  Peninsular\n",
      "tu:sing            0.34        0.20\n",
      "usted:sing         0.66        0.80\n",
      "vosotros:plur      0.20        0.21\n",
      "ustedes:plur       0.80        0.79\n"
     ]
    }
   ],
   "source": [
    "## Calculate proportional usage\n",
    "\n",
    "a_sing = a_tu + a_usted\n",
    "a_plur = a_vosotros + a_ustedes\n",
    "p_sing = p_tu + p_usted\n",
    "p_plur = p_vosotros + p_ustedes\n",
    "\n",
    "prop_dict_ampen = {'American': [round((a_tu/a_sing), 2), \n",
    "                          round((a_usted/a_sing), 2),\n",
    "                          round((a_vosotros/a_plur), 2),\n",
    "                          round((a_ustedes/a_plur), 2)],  \n",
    "             'Peninsular': [round((p_tu/p_sing), 2), \n",
    "                            round((p_usted/p_sing), 2), \n",
    "                            round((p_vosotros/p_plur), 2), \n",
    "                            round((p_ustedes/p_plur), 2)]}\n",
    "\n",
    "proportions = pd.DataFrame(prop_dict_ampen, index = ['tu:sing', 'usted:sing', 'vosotros:plur', 'ustedes:plur'])\n",
    "\n",
    "print(proportions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of newspaper vs. other domains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load corpora files\n",
    "\n",
    "am_noticias_corpus = []\n",
    "am_other_corpus =[]\n",
    "pen_noticias_corpus = []\n",
    "pen_other_corpus = []\n",
    "\n",
    "for file in corpora_files:\n",
    "    myspreader = SpanishReader.SpanishPlaintextCorpusReader(corpora_dir, file)\n",
    "    text = myspreader.words()\n",
    "    if re.search('a-xix-', file) or re.search('a-xx-', file):\n",
    "        if re.search('a-xix-j-', file):\n",
    "            for t in text:\n",
    "                am_noticias_corpus.append(t)\n",
    "        else: # if anything other than noticias\n",
    "            for t in text:\n",
    "                am_other_corpus.append(t)\n",
    "    else: # if peninsular\n",
    "        if re.search('p-xix-j-', file):\n",
    "            for t in text:\n",
    "                pen_noticias_corpus.append(t)\n",
    "        else: # if anything other than noticias\n",
    "            for t in text:\n",
    "                pen_other_corpus.append(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "537999\n",
      "1384410\n",
      "302096\n",
      "1384456\n"
     ]
    }
   ],
   "source": [
    "## Get size of corpora\n",
    "\n",
    "a_not_n = len(am_noticias_corpus)\n",
    "print(a_not_n)\n",
    "a_oth_n = len(am_other_corpus)\n",
    "print(a_oth_n)\n",
    "p_not_n = len(pen_noticias_corpus)\n",
    "print(p_not_n)\n",
    "p_oth_n = len(pen_other_corpus)\n",
    "print(p_oth_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NB: other corpora are much larger than newspaper (noticias) corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Search for ust/vos in corpora\n",
    "\n",
    "a_not_tu = 0\n",
    "a_oth_tu = 0\n",
    "p_not_tu = 0\n",
    "p_oth_tu = 0\n",
    "a_not_usted = 0\n",
    "a_oth_usted = 0\n",
    "p_not_usted = 0\n",
    "p_oth_usted = 0\n",
    "a_not_vosotros = 0\n",
    "a_oth_vosotros = 0\n",
    "p_not_vosotros = 0\n",
    "p_oth_vosotros = 0\n",
    "a_not_ustedes = 0\n",
    "a_oth_ustedes = 0\n",
    "p_not_ustedes = 0\n",
    "p_oth_ustedes = 0\n",
    "a_not_yo = 0\n",
    "a_oth_yo = 0\n",
    "p_not_yo = 0\n",
    "p_oth_yo = 0\n",
    "a_not_nosotros = 0\n",
    "a_oth_nosotros = 0\n",
    "p_not_nosotros = 0\n",
    "p_oth_nosotros = 0\n",
    "\n",
    "for word in tu:\n",
    "    a_not_tu += count_word(word, am_noticias_corpus)\n",
    "    a_oth_tu += count_word(word, am_other_corpus)\n",
    "    p_not_tu += count_word(word, pen_noticias_corpus)\n",
    "    p_oth_tu += count_word(word, pen_other_corpus)\n",
    "\n",
    "for word in usted:\n",
    "    a_not_usted += count_word(word, am_noticias_corpus)\n",
    "    a_oth_usted += count_word(word, am_other_corpus)\n",
    "    p_not_usted += count_word(word, pen_noticias_corpus)\n",
    "    p_oth_usted += count_word(word, pen_other_corpus)\n",
    "\n",
    "for word in vosotros:\n",
    "    a_not_vosotros += count_word(word, am_noticias_corpus)\n",
    "    a_oth_vosotros += count_word(word, am_other_corpus)\n",
    "    p_not_vosotros += count_word(word, pen_noticias_corpus)\n",
    "    p_oth_vosotros += count_word(word, pen_other_corpus)\n",
    "\n",
    "for word in ustedes:\n",
    "    a_not_ustedes += count_word(word, am_noticias_corpus)\n",
    "    a_oth_ustedes += count_word(word, am_other_corpus)\n",
    "    p_not_ustedes += count_word(word, pen_noticias_corpus)\n",
    "    p_oth_ustedes += count_word(word, pen_other_corpus)\n",
    "    \n",
    "for word in yo:\n",
    "    a_not_yo += count_word(word, am_noticias_corpus)\n",
    "    a_oth_yo += count_word(word, am_other_corpus)\n",
    "    p_not_yo += count_word(word, pen_noticias_corpus)\n",
    "    p_oth_yo += count_word(word, pen_other_corpus)\n",
    "    \n",
    "for word in nosotros:\n",
    "    a_not_nosotros += count_word(word, am_noticias_corpus)\n",
    "    a_oth_nosotros += count_word(word, am_other_corpus)\n",
    "    p_not_nosotros += count_word(word, pen_noticias_corpus)\n",
    "    p_oth_nosotros += count_word(word, pen_other_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Am-Noticias  Am-Other  Pen-Noticias  Pen-Other\n",
      "tu                 72      1104            25        757\n",
      "usted              54      2196             7       3110\n",
      "vosotros           51        46             6         84\n",
      "ustedes            94       283            12        317\n",
      "yo                762      3680            76       3940\n",
      "nosotros          248       574            83        553\n",
      "\n",
      "Size of American-Noticias corpus: 537999\n",
      "\n",
      "Size of American-Other corpus: 1384410\n",
      "\n",
      "Size of Peninsular-Noticias corpus: 302096\n",
      "\n",
      "Size of Peninsular-Other corpus: 1384456\n",
      "\n",
      "Am-Noticias     1281\n",
      "Am-Other        7883\n",
      "Pen-Noticias     209\n",
      "Pen-Other       8761\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "## Get total counts\n",
    "\n",
    "counts_dict_noticias = {'Am-Noticias': [a_not_tu, a_not_usted, a_not_vosotros, a_not_ustedes, a_not_yo, a_not_nosotros], \n",
    "               'Am-Other': [a_oth_tu, a_oth_usted, a_oth_vosotros, a_oth_ustedes, a_oth_yo, a_oth_nosotros],\n",
    "               'Pen-Noticias': [p_not_tu, p_not_usted, p_not_vosotros, p_not_ustedes, p_not_yo, p_not_nosotros],\n",
    "               'Pen-Other': [p_oth_tu, p_oth_usted, p_oth_vosotros, p_oth_ustedes, p_oth_yo, p_oth_nosotros]}\n",
    "\n",
    "total_counts_noticias = pd.DataFrame(counts_dict_noticias, index = ['tu', 'usted', 'vosotros', 'ustedes', 'yo', 'nosotros'] )\n",
    "\n",
    "print(total_counts_noticias)\n",
    "print('\\nSize of American-Noticias corpus: ' + str(a_not_n))\n",
    "print('\\nSize of American-Other corpus: ' + str(a_oth_n))\n",
    "print('\\nSize of Peninsular-Noticias corpus: ' + str(p_not_n))\n",
    "print('\\nSize of Peninsular-Other corpus: ' + str(p_oth_n) +'\\n')\n",
    "print(total_counts_noticias.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Am-Noticias  Am-Other  Pen-Noticias  Pen-Other\n",
      "tu:sing               0.57      0.33          0.78       0.20\n",
      "usted:sing            0.43      0.67          0.22       0.80\n",
      "vosotros:plur         0.35      0.14          0.33       0.21\n",
      "ustedes:plur          0.65      0.86          0.67       0.79\n"
     ]
    }
   ],
   "source": [
    "## Calculate proportional usage\n",
    "\n",
    "a_not_sing = a_not_tu + a_not_usted\n",
    "a_oth_sing = a_oth_tu + a_oth_usted\n",
    "a_not_plur = a_not_vosotros + a_not_ustedes\n",
    "a_oth_plur = a_oth_vosotros + a_oth_ustedes\n",
    "p_not_sing = p_not_tu + p_not_usted\n",
    "p_oth_sing = p_oth_tu + p_oth_usted\n",
    "p_not_plur = p_not_vosotros + p_not_ustedes\n",
    "p_oth_plur = p_oth_vosotros + p_oth_ustedes\n",
    "\n",
    "prop_dict_noticias = {'Am-Noticias': [round((a_not_tu/a_not_sing), 2), \n",
    "                                    round((a_not_usted/a_not_sing), 2),\n",
    "                                    round((a_not_vosotros/a_not_plur), 2),\n",
    "                                    round((a_not_ustedes/a_not_plur), 2)],\n",
    "                    'Am-Other': [round((a_oth_tu/a_oth_sing), 2), \n",
    "                                   round((a_oth_usted/a_oth_sing), 2),\n",
    "                                   round((a_oth_vosotros/a_oth_plur), 2),\n",
    "                                   round((a_oth_ustedes/a_oth_plur), 2)],  \n",
    "                    'Pen-Noticias': [round((p_not_tu/p_not_sing), 2), \n",
    "                                     round((p_not_usted/p_not_sing), 2), \n",
    "                                     round((p_not_vosotros/p_not_plur), 2), \n",
    "                                     round((p_not_ustedes/p_not_plur), 2)],\n",
    "                    'Pen-Other': [round((p_oth_tu/p_oth_sing), 2), \n",
    "                                     round((p_oth_usted/p_oth_sing), 2), \n",
    "                                     round((p_oth_vosotros/p_oth_plur), 2), \n",
    "                                     round((p_oth_ustedes/p_oth_plur), 2)]}\n",
    "\n",
    "proportions_noticias = pd.DataFrame(prop_dict_noticias, index = ['tu:sing', 'usted:sing', 'vosotros:plur', 'ustedes:plur'])\n",
    "\n",
    "print(proportions_noticias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of non-fiction vs. fiction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load corpora files\n",
    "\n",
    "am_nonfiction_corpus = []\n",
    "am_fiction_corpus =[]\n",
    "pen_nonfiction_corpus = []\n",
    "pen_fiction_corpus = []\n",
    "\n",
    "for file in corpora_files:\n",
    "    myspreader = SpanishReader.SpanishPlaintextCorpusReader(corpora_dir, file)\n",
    "    text = myspreader.words()\n",
    "    if re.search('a-xix-', file) or re.search('a-xx-', file):\n",
    "        if re.search('a-xix-j-', file) or re.search('a-xix-n-', file) or re.search('a-xx-n-', file):\n",
    "            for t in text:\n",
    "                am_nonfiction_corpus.append(t)\n",
    "        else: # if anything other than non-fiction (i.e., ficcion)\n",
    "            for t in text:\n",
    "                am_fiction_corpus.append(t)\n",
    "    else: # if peninsular\n",
    "        if re.search('p-xix-j-', file) or re.search('p-xix-n-', file):\n",
    "            for t in text:\n",
    "                pen_nonfiction_corpus.append(t)\n",
    "        else: # if anything other than non-fiction (i.e., ficcion)\n",
    "            for t in text:\n",
    "                pen_fiction_corpus.append(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "882545\n",
      "1039864\n",
      "411780\n",
      "1274772\n"
     ]
    }
   ],
   "source": [
    "## Get size of corpora\n",
    "\n",
    "a_non_n = len(am_nonfiction_corpus)\n",
    "print(a_non_n)\n",
    "a_fic_n = len(am_fiction_corpus)\n",
    "print(a_fic_n)\n",
    "p_non_n = len(pen_nonfiction_corpus)\n",
    "print(p_non_n)\n",
    "p_fic_n = len(pen_fiction_corpus)\n",
    "print(p_fic_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Search for ust/vos in corpora\n",
    "\n",
    "a_non_tu = 0\n",
    "a_fic_tu = 0\n",
    "p_non_tu = 0\n",
    "p_fic_tu = 0\n",
    "a_non_usted = 0\n",
    "a_fic_usted = 0\n",
    "p_non_usted = 0\n",
    "p_fic_usted = 0\n",
    "a_non_vosotros = 0\n",
    "a_fic_vosotros = 0\n",
    "p_non_vosotros = 0\n",
    "p_fic_vosotros = 0\n",
    "a_non_ustedes = 0\n",
    "a_fic_ustedes = 0\n",
    "p_non_ustedes = 0\n",
    "p_fic_ustedes = 0\n",
    "a_non_yo = 0\n",
    "a_fic_yo = 0\n",
    "p_non_yo = 0\n",
    "p_fic_yo = 0\n",
    "a_non_nosotros = 0\n",
    "a_fic_nosotros = 0\n",
    "p_non_nosotros = 0\n",
    "p_fic_nosotros = 0\n",
    "\n",
    "for word in tu:\n",
    "    a_non_tu += count_word(word, am_nonfiction_corpus)\n",
    "    a_fic_tu += count_word(word, am_fiction_corpus)\n",
    "    p_non_tu += count_word(word, pen_nonfiction_corpus)\n",
    "    p_fic_tu += count_word(word, pen_fiction_corpus)\n",
    "\n",
    "for word in usted:\n",
    "    a_non_usted += count_word(word, am_nonfiction_corpus)\n",
    "    a_fic_usted += count_word(word, am_fiction_corpus)\n",
    "    p_non_usted += count_word(word, pen_nonfiction_corpus)\n",
    "    p_fic_usted += count_word(word, pen_fiction_corpus)\n",
    "\n",
    "for word in vosotros:\n",
    "    a_non_vosotros += count_word(word, am_nonfiction_corpus)\n",
    "    a_fic_vosotros += count_word(word, am_fiction_corpus)\n",
    "    p_non_vosotros += count_word(word, pen_nonfiction_corpus)\n",
    "    p_fic_vosotros += count_word(word, pen_fiction_corpus)\n",
    "\n",
    "for word in ustedes:\n",
    "    a_non_ustedes += count_word(word, am_nonfiction_corpus)\n",
    "    a_fic_ustedes += count_word(word, am_fiction_corpus)\n",
    "    p_non_ustedes += count_word(word, pen_nonfiction_corpus)\n",
    "    p_fic_ustedes += count_word(word, pen_fiction_corpus)\n",
    "    \n",
    "for word in yo:\n",
    "    a_non_yo += count_word(word, am_nonfiction_corpus)\n",
    "    a_fic_yo += count_word(word, am_fiction_corpus)\n",
    "    p_non_yo += count_word(word, pen_nonfiction_corpus)\n",
    "    p_fic_yo += count_word(word, pen_fiction_corpus)\n",
    "    \n",
    "for word in nosotros:\n",
    "    a_non_nosotros += count_word(word, am_nonfiction_corpus)\n",
    "    a_fic_nosotros += count_word(word, am_fiction_corpus)\n",
    "    p_non_nosotros += count_word(word, pen_nonfiction_corpus)\n",
    "    p_fic_nosotros += count_word(word, pen_fiction_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Am-Nonfiction  Am-Fiction  Pen-Nonfiction  Pen-Fiction\n",
      "tu                  145        1031              28          754\n",
      "usted               368        1882              89         3028\n",
      "vosotros             69          28               6           84\n",
      "ustedes             128         249              16          313\n",
      "yo                 1424        3018             355         3661\n",
      "nosotros            403         419             188          448\n",
      "\n",
      "Size of American-Nonfiction corpus: 882545\n",
      "\n",
      "Size of American-Fiction corpus: 1039864\n",
      "\n",
      "Size of Peninsular-Nonfiction corpus: 411780\n",
      "\n",
      "Size of Peninsular-Fiction corpus: 1274772\n",
      "\n",
      "Am-Nonfiction     2537\n",
      "Am-Fiction        6627\n",
      "Pen-Nonfiction     682\n",
      "Pen-Fiction       8288\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "## Get total counts\n",
    "\n",
    "counts_dict_domain = {'Am-Nonfiction': [a_non_tu, a_non_usted, a_non_vosotros, a_non_ustedes, a_non_yo, a_non_nosotros], \n",
    "               'Am-Fiction': [a_fic_tu, a_fic_usted, a_fic_vosotros, a_fic_ustedes, a_fic_yo, a_fic_nosotros],\n",
    "               'Pen-Nonfiction': [p_non_tu, p_non_usted, p_non_vosotros, p_non_ustedes, p_non_yo, p_non_nosotros],\n",
    "               'Pen-Fiction': [p_fic_tu, p_fic_usted, p_fic_vosotros, p_fic_ustedes, p_fic_yo, p_fic_nosotros]}\n",
    "\n",
    "total_counts_domain = pd.DataFrame(counts_dict_domain, index = ['tu', 'usted', 'vosotros', 'ustedes', 'yo', 'nosotros'] )\n",
    "\n",
    "print(total_counts_domain)\n",
    "print('\\nSize of American-Nonfiction corpus: ' + str(a_non_n))\n",
    "print('\\nSize of American-Fiction corpus: ' + str(a_fic_n))\n",
    "print('\\nSize of Peninsular-Nonfiction corpus: ' + str(p_non_n))\n",
    "print('\\nSize of Peninsular-Fiction corpus: ' + str(p_fic_n) +'\\n')\n",
    "print(total_counts_domain.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Am-Nonfiction  Am-Fiction  Pen-Nonfiction  Pen-Fiction\n",
      "tu:sing                 0.28        0.35            0.24         0.20\n",
      "usted:sing              0.72        0.65            0.76         0.80\n",
      "vosotros:plur           0.35        0.10            0.27         0.21\n",
      "ustedes:plur            0.65        0.90            0.73         0.79\n"
     ]
    }
   ],
   "source": [
    "## Calculate proportional usage\n",
    "\n",
    "a_non_sing = a_non_tu + a_non_usted\n",
    "a_fic_sing = a_fic_tu + a_fic_usted\n",
    "a_non_plur = a_non_vosotros + a_non_ustedes\n",
    "a_fic_plur = a_fic_vosotros + a_fic_ustedes\n",
    "p_non_sing = p_non_tu + p_non_usted\n",
    "p_fic_sing = p_fic_tu + p_fic_usted\n",
    "p_non_plur = p_non_vosotros + p_non_ustedes\n",
    "p_fic_plur = p_fic_vosotros + p_fic_ustedes\n",
    "\n",
    "prop_dict_domain = {'Am-Nonfiction': [round((a_non_tu/a_non_sing), 2), \n",
    "                                    round((a_non_usted/a_non_sing), 2),\n",
    "                                    round((a_non_vosotros/a_non_plur), 2),\n",
    "                                    round((a_non_ustedes/a_non_plur), 2)],\n",
    "                    'Am-Fiction': [round((a_fic_tu/a_fic_sing), 2), \n",
    "                                   round((a_fic_usted/a_fic_sing), 2),\n",
    "                                   round((a_fic_vosotros/a_fic_plur), 2),\n",
    "                                   round((a_fic_ustedes/a_fic_plur), 2)],  \n",
    "                    'Pen-Nonfiction': [round((p_non_tu/p_non_sing), 2), \n",
    "                                     round((p_non_usted/p_non_sing), 2), \n",
    "                                     round((p_non_vosotros/p_non_plur), 2), \n",
    "                                     round((p_non_ustedes/p_non_plur), 2)],\n",
    "                    'Pen-Fiction': [round((p_fic_tu/p_fic_sing), 2), \n",
    "                                     round((p_fic_usted/p_fic_sing), 2), \n",
    "                                     round((p_fic_vosotros/p_fic_plur), 2), \n",
    "                                     round((p_fic_ustedes/p_fic_plur), 2)]}\n",
    "\n",
    "proportions_domain = pd.DataFrame(prop_dict_domain, index = ['tu:sing', 'usted:sing', 'vosotros:plur', 'ustedes:plur'])\n",
    "\n",
    "print(proportions_domain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sanity check against Raul's original calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nos_vos_uds (AM SP first in each pair)\n",
    "\n",
    "[[569, 553], [[46, 84], [283, 317]], [1383512, 1383502]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "American Spanish: 1384410\n",
      "\n",
      "Peninsular Spanish: 1384456\n"
     ]
    }
   ],
   "source": [
    "## get counts for corpus w/o newspapers\n",
    "\n",
    "print('American Spanish: ' + str(a_oth_n))\n",
    "print('\\nPeninsular Spanish: ' + str(p_oth_n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "American Spanish:\n",
      "nos: 574\n",
      "vos: 46\n",
      "uds: 283\n",
      "\n",
      "Peninsular Spanish:\n",
      "nos: 553\n",
      "vos: 84\n",
      "uds: 317\n"
     ]
    }
   ],
   "source": [
    "## get nos, vos, uds counts\n",
    "\n",
    "print('American Spanish:' + \n",
    "     '\\nnos: ' + str(a_oth_nosotros) +\n",
    "     '\\nvos: ' + str(a_oth_vosotros) +\n",
    "     '\\nuds: ' + str(a_oth_ustedes) + '\\n')\n",
    "\n",
    "print('Peninsular Spanish:' +\n",
    "     '\\nnos: ' + str(p_oth_nosotros) +\n",
    "     '\\nvos: ' + str(p_oth_vosotros) +\n",
    "     '\\nuds: ' + str(p_oth_ustedes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Counts are almost identical - nos count is higher here because this analysis included use of nos abbreviation 'nos.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
